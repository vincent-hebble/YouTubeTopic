{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bertopic\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np# Text preprocessiong\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()# Topic model\n",
    "from bertopic import BERTopic# Dimension reduction\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(r\"Final_Data\\Data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to check for special characters\n",
    "def has_special_characters(word):\n",
    "    return any(char in word for char in \"!@#$%^&*()_-+=<>?,./;:[]{}|\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "def graph_net(df):\n",
    "\n",
    "\n",
    "\n",
    "    df_test = df['review_lemmatized']\n",
    "\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Create dictionaries to store the counts of nouns, verbs, and adjectives\n",
    "    noun_counts = defaultdict(int)\n",
    "    verb_counts = defaultdict(int)\n",
    "    adjective_counts = defaultdict(int)\n",
    "\n",
    "    # Create a co-occurrence matrix for nouns, verbs, and adjectives\n",
    "    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Process each title and count nouns, verbs, and adjectives\n",
    "    for title in df_test:\n",
    "        if isinstance(title, str):\n",
    "            doc = nlp(title)\n",
    "        else:\n",
    "            continue\n",
    "        title_str = str(title)\n",
    "        doc = nlp(title)\n",
    "        nouns = [token.text for token in doc if token.pos_ == 'NOUN' and not has_special_characters(token.text)]\n",
    "        verbs = [token.text for token in doc if token.pos_ == 'VERB' and not has_special_characters(token.text)]\n",
    "        adjectives = [token.text for token in doc if token.pos_ == 'ADJ' and not has_special_characters(token.text)]\n",
    "\n",
    "        for noun in nouns:\n",
    "            noun_counts[noun] += 1\n",
    "        for verb in verbs:\n",
    "            verb_counts[verb] += 1\n",
    "        for adjective in adjectives:\n",
    "            adjective_counts[adjective] += 1\n",
    "\n",
    "        # Construct co-occurrence matrix for noun-verb-adjective connections\n",
    "        for noun in nouns:\n",
    "            for verb in verbs:\n",
    "                co_occurrence_matrix[noun][verb] += 1\n",
    "        for noun in nouns:\n",
    "            for adjective in adjectives:\n",
    "                co_occurrence_matrix[noun][adjective] += 1\n",
    "        for verb in verbs:\n",
    "            for adjective in adjectives:\n",
    "                co_occurrence_matrix[verb][adjective] += 1\n",
    "\n",
    "    # Create a graph network\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes (nouns, verbs, and adjectives) with different colors\n",
    "    for word, count in noun_counts.items():\n",
    "        G.add_node(word, count=count, pos='NOUN', color='lightblue')\n",
    "    for word, count in verb_counts.items():\n",
    "        G.add_node(word, count=count, pos='VERB', color='green')\n",
    "    for word, count in adjective_counts.items():\n",
    "        G.add_node(word, count=count, pos='ADJ', color='red')\n",
    "\n",
    "    # Add edges (co-occurrence relationships between nouns, verbs, and adjectives)\n",
    "    for noun, cooccur in co_occurrence_matrix.items():\n",
    "        for word, count in cooccur.items():\n",
    "            # Ensure that connections are only between different types of nodes\n",
    "            if G.nodes[noun]['pos'] != G.nodes[word]['pos']:\n",
    "                G.add_edge(noun, word, weight=count)\n",
    "\n",
    "    # Select the top 50 nodes by degree\n",
    "    # top_nodes = sorted(G.nodes, key=lambda node: -G.degree(node))\n",
    "    top_nodes = sorted(G.nodes, key=lambda node: -G.degree(node))[:50]\n",
    "\n",
    "    # Create a subgraph with the top nodes\n",
    "    H = G.subgraph(top_nodes)\n",
    "\n",
    "    # Check and remove isolated nodes from the subgraph by creating a copy\n",
    "    isolated_nodes = [node for node in H.nodes if H.degree(node) == 0]\n",
    "    H_copy = H.copy()\n",
    "    H_copy.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    # Use Kamada-Kawai layout to achieve better node positioning for the copy\n",
    "    pos = nx.kamada_kawai_layout(H_copy)\n",
    "\n",
    "    # Plot the subgraph with different node colors\n",
    "    node_colors = [H.nodes[node]['color'] for node in H_copy]\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw_networkx_nodes(H_copy, pos, node_size=200, node_color=node_colors)\n",
    "    nx.draw_networkx_edges(H_copy, pos, edge_color='gray', alpha=0.7)\n",
    "    nx.draw_networkx_labels(H_copy, pos, font_size=8, font_color='black', font_weight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Co-occurrence Network of Top 50 Nouns, Verbs, and Adjectives in Titles\")\n",
    "    plt.show()\n",
    "    return G.nodes(data=True),G.edges(data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_data(raw_data,topic):\n",
    "    nodes_df = pd.DataFrame(columns=[\"id\", \"count\", \"pos\", \"color\",'topic'])\n",
    "    for node, data in raw_data:\n",
    "        nodes_df = pd.concat([nodes_df, pd.DataFrame([{\n",
    "            \"id\": node,\n",
    "            \"count\": data[\"count\"],\n",
    "            \"pos\": data[\"pos\"],\n",
    "            \"color\": data[\"color\"],\n",
    "            \"topic\": topic\n",
    "        }])], ignore_index=True)\n",
    "    return nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_data(raw_data,topic):\n",
    "    edges_df = pd.DataFrame(columns=[\"source\", \"target\", \"weight\",'topic'])\n",
    "    for source, target, data in raw_data:\n",
    "        edges_df = pd.concat([edges_df, pd.DataFrame([{\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "            \"weight\": data[\"weight\"],\n",
    "            \"topic\": topic\n",
    "\n",
    "        }])], ignore_index=True)\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_topics(df): \n",
    "    edge_df=pd.DataFrame(columns=[\"source\", \"target\", \"weight\",'topic'])\n",
    "    node_df=pd.DataFrame(columns=[\"id\", \"count\", \"pos\", \"color\",'topic'])\n",
    "    num_topics=len(df.topic.unique())\n",
    "    for topic in range(num_topics):\n",
    "        topic_data=df[df.topic==topic].reset_index()\n",
    "        nodes,edges=graph_net(topic_data)\n",
    "        \n",
    "        edge_topic_data=edge_data(edges,topic)\n",
    "        edge_df=pd.concat([edge_df,edge_topic_data])\n",
    "        \n",
    "        node_topic_data=node_data(nodes,topic)\n",
    "        node_df=pd.concat([node_df,node_topic_data])\n",
    "        \n",
    "    return node_df,edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    node_df,edge_df= all_topics(df)\n",
    "    edge_df.to_csv(r'Final_Data\\edges_df.csv')\n",
    "    node_df.to_csv(r'Final_Data\\node_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
