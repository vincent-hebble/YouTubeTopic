{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np# Text preprocessiong\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()# Topic model\n",
    "from bertopic import BERTopic# Dimension reduction\n",
    "from umap import UMAP\n",
    "import bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741a9bb",
   "metadata": {},
   "source": [
    "## READ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"Data\\US_youtube_trending_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e191866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates(subset=['title'],keep='first')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# Remove stopwords\n",
    "df['review_without_stopwords'] = df['title'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))\n",
    "df['review_lemmatized'] = df['review_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))\n",
    "\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "# Use the str.replace method to remove non-alphanumeric characters and keep only alphanumeric characters\n",
    "df['review_lemmatized'] = df['review_lemmatized'].str.replace(pattern, '', regex=True)\n",
    "df['review_lemmatized'] = df['review_lemmatized'].str.upper()\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10cf78",
   "metadata": {},
   "source": [
    "## BERTOPIC MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=50, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.65, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=False)# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['review_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326ab9d",
   "metadata": {},
   "source": [
    "## TOP 30 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df.copy()\n",
    "topic_prediction = topic_model.topics_[:]\n",
    "df_final['topic']=topic_prediction\n",
    "\n",
    "test=topic_model.get_topic_info()\n",
    "df_final['Docs']=test['Representative_Docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_final[(df_final['topic'] >= 0) & (df_final['topic'] <= 30)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe48a90",
   "metadata": {},
   "source": [
    "## TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(df_final['review_lemmatized'], \n",
    "                                                df_final['trending_date'], \n",
    "                                                global_tuning=True, \n",
    "                                              evolution_tuning=True\n",
    "                                               ,nr_bins=30)\n",
    "time_series_data=topics_over_time[(topics_over_time['Topic'] >= 0) & (topics_over_time['Topic'] <= 30)]\n",
    "time_series_data['Timestamp'] = time_series_data['Timestamp'].dt.tz_localize(None)\n",
    "time_series_data['Timestamp'] = time_series_data['Timestamp'] .dt.date\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e63033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_topics_over_time(topics_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0321971",
   "metadata": {},
   "source": [
    "## INTERTOPIC DISTANCE MAP\n",
    "### MODIFIED FROM ORIGINAL BERTOPIC GITHUB TO OUTPUT DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from typing import List, Union\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_topics_data(topic_model,\n",
    "                     topics: List[int] = None,\n",
    "                     top_n_topics: int = None,\n",
    "                     custom_labels: Union[bool, str] = False,\n",
    "                     title: str = \"<b>Intertopic Distance Map</b>\",\n",
    "                     width: int = 650,\n",
    "                     height: int = 650) -> go.Figure:\n",
    "    \"\"\" Visualize topics, their sizes, and their corresponding words\n",
    "\n",
    "    This visualization is highly inspired by LDAvis, a great visualization\n",
    "    technique typically reserved for LDA.\n",
    "\n",
    "    Arguments:\n",
    "        topic_model: A fitted BERTopic instance.\n",
    "        topics: A selection of topics to visualize\n",
    "        top_n_topics: Only select the top n most frequent topics\n",
    "        custom_labels: If bool, whether to use custom topic labels that were defined using \n",
    "                       `topic_model.set_topic_labels`.\n",
    "                       If `str`, it uses labels from other aspects, e.g., \"Aspect1\".\n",
    "        title: Title of the plot.\n",
    "        width: The width of the figure.\n",
    "        height: The height of the figure.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    To visualize the topics simply run:\n",
    "\n",
    "    ```python\n",
    "    topic_model.visualize_topics()\n",
    "    ```\n",
    "\n",
    "    Or if you want to save the resulting figure:\n",
    "\n",
    "    ```python\n",
    "    fig = topic_model.visualize_topics()\n",
    "    fig.write_html(\"path/to/file.html\")\n",
    "    ```\n",
    "    <iframe src=\"../../getting_started/visualization/viz.html\"\n",
    "    style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe>\n",
    "    \"\"\"\n",
    "    # Select topics based on top_n and topics args\n",
    "    freq_df = topic_model.get_topic_freq()\n",
    "    freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "    if topics is not None:\n",
    "        topics = list(topics)\n",
    "    elif top_n_topics is not None:\n",
    "        topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "    else:\n",
    "        topics = sorted(freq_df.Topic.to_list())\n",
    "\n",
    "    # Extract topic words and their frequencies\n",
    "    topic_list = sorted(topics)\n",
    "    frequencies = [topic_model.topic_sizes_[topic] for topic in topic_list]\n",
    "    if isinstance(custom_labels, str):\n",
    "        words = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in topic_list]\n",
    "        words = [\"_\".join([label[0] for label in labels[:4]]) for labels in words]\n",
    "        words = [label if len(label) < 30 else label[:27] + \"...\" for label in words]\n",
    "    elif custom_labels and topic_model.custom_labels_ is not None:\n",
    "        words = [topic_model.custom_labels_[topic + topic_model._outliers] for topic in topic_list]\n",
    "    else:\n",
    "        words = [\" | \".join([word[0] for word in topic_model.get_topic(topic)[:5]]) for topic in topic_list]\n",
    "\n",
    "    # Embed c-TF-IDF into 2D\n",
    "    all_topics = sorted(list(topic_model.get_topics().keys()))\n",
    "    indices = np.array([all_topics.index(topic) for topic in topics])\n",
    "\n",
    "    if topic_model.topic_embeddings_ is not None:\n",
    "        embeddings = topic_model.topic_embeddings_[indices]\n",
    "        embeddings = UMAP(n_neighbors=2, n_components=2, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    else:\n",
    "        embeddings = topic_model.c_tf_idf_.toarray()[indices]\n",
    "        embeddings = MinMaxScaler().fit_transform(embeddings)\n",
    "        embeddings = UMAP(n_neighbors=2, n_components=2, metric='hellinger', random_state=42).fit_transform(embeddings)\n",
    "\n",
    "    # Visualize with plotly\n",
    "    df = pd.DataFrame({\"x\": embeddings[:, 0], \"y\": embeddings[:, 1],\n",
    "                       \"Topic\": topic_list, \"Words\": words, \"Size\": frequencies})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Distance DF\n",
    "# topic_model.visualize_topics(topics=list(range(0,31)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403a86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_distance_data=visualize_topics_data(topic_model,topics=list(range(0,31)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb922c8",
   "metadata": {},
   "source": [
    "# BIGRAM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.collocations import * \n",
    "\n",
    "\n",
    "def Bigram_Data(filtered_df):\n",
    "\n",
    "    list_documents = filtered_df['review_lemmatized'].apply(lambda x: x.split()).tolist() \n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures() \n",
    "    bigram_finder = BigramCollocationFinder.from_documents(list_documents) \n",
    "    bigram_finder.apply_freq_filter(3) \n",
    "    bigrams = bigram_finder.nbest(bigram_measures.raw_freq,20) \n",
    "    scores = bigram_finder.score_ngrams(bigram_measures.raw_freq) \n",
    "    ngram = list(bigram_finder.ngram_fd.items()) \n",
    "    ngram.sort(key=lambda item: item[-1], reverse=True) \n",
    "\n",
    "    frequency = [(\" \".join(k), v) for k,v in ngram] \n",
    "    df_bigrams=pd.DataFrame(frequency) \n",
    "    df_bigrams.rename(columns={0: \"Term\", 1: \"Frequency\"},inplace=True)\n",
    "    df_bigrams= df_bigrams.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "    return df_bigrams.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da37d2",
   "metadata": {},
   "source": [
    "## Individual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e726f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "def Word_Frequency(filtered_df):\n",
    "\n",
    "    list_documents = filtered_df['review_lemmatized'].apply(lambda x: x.split()).tolist()\n",
    "    all_words = [word for sublist in list_documents for word in sublist]\n",
    "    word_freq = FreqDist(all_words)\n",
    "    df_word_frequencies = pd.DataFrame(word_freq.items(), columns=['Term', 'Frequency'])\n",
    "    df_word_frequencies = df_word_frequencies.sort_values(by='Frequency', ascending=False)\n",
    "    df_word_frequencies= df_word_frequencies.sort_values(by='Frequency', ascending=False)\n",
    "    return df_word_frequencies.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4f13f",
   "metadata": {},
   "source": [
    "## CREATE DATAFRAME THAT CONTAINS BIGRAM & INDIVIDUAL WORD FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Freqeuncy_all_topics(df): \n",
    "    Frequency_df=pd.DataFrame(columns=[\"Term\", \"Frequency\", \"Topic\",'Bigram'])\n",
    "    num_topics=len(df.topic.unique())\n",
    "    for topic in range(num_topics):\n",
    "        topic_data=df[df.topic==topic].reset_index()\n",
    "        word_df=Word_Frequency(topic_data)    \n",
    "        bigram_df=Bigram_Data(topic_data)\n",
    "        \n",
    "        word_df['Topic']=topic\n",
    "        bigram_df['Topic']=topic\n",
    "        word_df['Bigram']='0'\n",
    "        bigram_df['Bigram']='1'\n",
    "        \n",
    "        combined_df=pd.concat([bigram_df,word_df])\n",
    "        Frequency_df=pd.concat([Frequency_df,combined_df])\n",
    "\n",
    "        \n",
    "    return Frequency_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Topic_frequency_df=Freqeuncy_all_topics(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a4dbe",
   "metadata": {},
   "source": [
    "## INTERTOPIC FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bef89d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mean_by_category = filtered_df.groupby('topic')['view_count'].mean().reset_index()\n",
    "mean_by_category.sort_values(by='view_count', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4e69c",
   "metadata": {},
   "source": [
    "## ANOVA & POST HOC TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580848ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "### ANOVA PREP\n",
    "data_frames = [filtered_df[filtered_df['topic'] == topic]['view_count'] for topic in filtered_df['topic'].unique()]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(*data_frames)\n",
    "\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print(\"There is significant evidence that at least one topic has a different view count.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in view counts between topics.\")\n",
    "\n",
    "\n",
    "## POST HOC TEST: PAIRWISE TUKEY    \n",
    "    \n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "mc = MultiComparison(filtered_df['view_count'], filtered_df['topic'])\n",
    "result = mc.tukeyhsd()\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f25c26",
   "metadata": {},
   "source": [
    "# FINAL DATA OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa61651",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_excel(r\"Final_Data\\Data.xlsx\")\n",
    "\n",
    "time_series_data.to_excel(r\"Final_Data\\Time_Series.xlsx\",index=False)\n",
    "time_series_data.to_csv(r\"Final_Data\\Time_Series.csv\",index=False)\n",
    "\n",
    "topic_distance_data.to_excel(r\"Final_Data\\Distance_Data.xlsx\")\n",
    "topic_distance_data.to_csv(r\"Final_Data\\Distance_Data.csv\")\n",
    "\n",
    "Topic_frequency_df.to_csv(r'Final_Data\\Frequency_df.csv',index=False)\n",
    "\n",
    "mean_by_category.to_csv(r\"Final_Data\\topic_view.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
